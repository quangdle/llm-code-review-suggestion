{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"password\"\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\", \n",
    "    username=\"neo4j\", \n",
    "    password=\"123456789\",\n",
    "    enhanced_schema=True\n",
    ")\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"GPT key\"\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm, graph=graph, verbose=True,\n",
    "     allow_dangerous_requests=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "- **Developer**\n",
      "  - `availability`: INTEGER Min: 3, Max: 5\n",
      "  - `code_review_frequent`: INTEGER Min: 4, Max: 4\n",
      "  - `id`: STRING Available options: ['quang', 'sang', 'hani', 'duong', 'nhi']\n",
      "  - `name`: STRING Available options: ['Quang', 'Sang', 'Hani', 'Duong', 'Nhi']\n",
      "  - `responsiveness`: INTEGER Min: 2, Max: 5\n",
      "  - `review_score`: FLOAT Min: 50.0, Max: 99.0\n",
      "- **Skill**\n",
      "  - `id`: STRING Available options: ['js', 'py', 'reactjs', 'ts', 'cicd', 'platform', 'ruby']\n",
      "  - `name`: STRING Available options: ['JavaScript', 'Python', 'ReactJS', 'Typescript', 'CI/CD', 'Platform', 'Ruby']\n",
      "- **Team**\n",
      "  - `id`: STRING Available options: ['helios', 'konoha', 'web-platform']\n",
      "  - `name`: STRING Available options: ['Helios', 'Konoha', 'Web Platform']\n",
      "  - `domains`: LIST Min Size: 1, Max Size: 2\n",
      "- **PR**\n",
      "  - `id`: STRING Available options: ['PR1', 'PR2']\n",
      "  - `domains`: LIST Min Size: 1, Max Size: 1\n",
      "  - `description`: STRING Available options: ['Fixes a critical bug to ATS', 'Adds a new feature to SmartMatch']\n",
      "  - `timestamp`: STRING Available options: ['2024-12-15', '2024-12-14']\n",
      "  - `title`: STRING Available options: ['Fix Bug', 'Add Feature']\n",
      "- **File**\n",
      "  - `id`: STRING Available options: ['F1', 'F2', 'F3', 'F4']\n",
      "  - `name`: STRING Available options: ['file1.js', 'file2.py', 'file3.ts', 'file4.rb']\n",
      "- **Chunk**\n",
      "  - `id`: STRING Available options: ['4756b598aafbb6abe8703acfe0b364ee', 'f87dd4954f9beedb81effa93301522eb', '9579eba81e4eb8b7f2fcb35b38360470', 'ccf523d15d62c45ad7c506a430620f4d', '4ed66c4841dc7ff5f43ecc58365dadf6']\n",
      "  - `text`: STRING Available options: ['Which developers have the highest level of skill i', 'Which team own file1.js?', 'Which pull request affected file file1.js?', 'Which developer has the most contribution in ATS d', 'Find top 3 developers to review a PR created by Nh']\n",
      "  - `question`: STRING Available options: ['Which developers have the highest level of skill i', 'Which team own file1.js?', 'Which pull request affected file file1.js?', 'Which developer has the most contribution in ATS d', 'Find top 3 developers to review a PR created by Nh']\n",
      "  - `query`: STRING Available options: ['           MATCH (d:Developer)-[r:DEVELOPER_HAS_SK', '           MATCH (t:Team)-[:CODEOWNER_OF]->(f:File', '           MATCH (pr:PR)-[:PR_AFFECTS_FILE]->(f:Fi', '           MATCH (d:Developer)           OPTIONAL ', '           WITH [               \"Ruby\"           ]']\n",
      "Relationship properties:\n",
      "- **DEVELOPER_HAS_SKILL**\n",
      "  - `level: INTEGER` Min: 3, Max:  5\n",
      "- **DEVELOPER_REVIEWED**\n",
      "  - `review_score: INTEGER` Min: 50, Max:  85\n",
      "The relationships:\n",
      "(:Developer)-[:DEVELOPER_HAS_SKILL]->(:Skill)\n",
      "(:Developer)-[:DEVELOPER_CONTRIBUTED_TO]->(:PR)\n",
      "(:Developer)-[:IS_MEMBER_OF_TEAM]->(:Team)\n",
      "(:Developer)-[:DEVELOPER_REVIEWED]->(:PR)\n",
      "(:Developer)-[:LEADER_OF]->(:Team)\n",
      "(:Developer)-[:MENTOR]->(:Developer)\n",
      "(:Team)-[:CODEOWNER_OF]->(:File)\n",
      "(:PR)-[:PR_AFFECTS_FILE]->(:File)\n"
     ]
    }
   ],
   "source": [
    "graph.refresh_schema()\n",
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from typing import Annotated, List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    question: str\n",
    "\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    question: str\n",
    "    next_action: str\n",
    "    cypher_statement: str\n",
    "    cypher_errors: List[str]\n",
    "    database_records: List[dict]\n",
    "    steps: Annotated[List[str], add]\n",
    "\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    answer: str\n",
    "    steps: List[str]\n",
    "    cypher_statement: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "guardrails_system = \"\"\"\n",
    "As an intelligent assistant, your primary objective is to decide whether a given question is related to code contribution or not. \n",
    "If the question is related to code contribution, output \"code contribution\". Otherwise, output \"end\".\n",
    "To make this decision, assess the cntent of the question and determine if it refers to any developers, teams, skills, pull requests, code reviews, contributions, mentoring,...\n",
    "or related topics. Provide only the specified output: \"code contribution\" or \"end\".\n",
    "\"\"\"\n",
    "guardrails_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            guardrails_system,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\"{question}\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class GuardrailsOutput(BaseModel):\n",
    "    decision: Literal[\"code contribution\", \"end\"] = Field(\n",
    "        description=\"Decision on whether the question is related to code contribution\"\n",
    "    )\n",
    "\n",
    "\n",
    "guardrails_chain = guardrails_prompt | llm.with_structured_output(GuardrailsOutput)\n",
    "\n",
    "\n",
    "def guardrails(state: InputState) -> OverallState:\n",
    "    \"\"\"\n",
    "    Decides if the question is related to code contribution or not.\n",
    "    \"\"\"\n",
    "    guardrails_output = guardrails_chain.invoke({\"question\": state.get(\"question\")})\n",
    "    database_records = None\n",
    "    if guardrails_output.decision == \"end\":\n",
    "        database_records = \"This questions is not about code contributions or related info. Therefore I cannot answer this question.\"\n",
    "    return {\n",
    "        \"next_action\": guardrails_output.decision,\n",
    "        \"database_records\": database_records,\n",
    "        \"steps\": [\"guardrail\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_neo4j import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Which developers have the highest level of skill in ReactJS?\",\n",
    "        \"query\": \"\"\"\n",
    "          MATCH (d:Developer)-[r:DEVELOPER_HAS_SKILL]->(s:Skill)\n",
    "          WHERE s.name = \"ReactJS\"\n",
    "          RETURN d.name AS Developer, r.level AS SkillLevel\n",
    "          ORDER BY r.level DESC\n",
    "          LIMIT 5\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which team own file1.js?\",\n",
    "        \"query\": \"\"\"\n",
    "          MATCH (t:Team)-[:CODEOWNER_OF]->(f:File {name: \"file1.js\"})\n",
    "          RETURN t.name AS Team, f.name AS File, f.domain AS Domain\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which pull request affected file file1.js?\",\n",
    "        \"query\": \"\"\"\n",
    "          MATCH (pr:PR)-[:PR_AFFECTS_FILE]->(f:File {name: \"file1.js\"})\n",
    "          RETURN pr.title AS PullRequest, pr.description AS Description, pr.timestamp AS Timestamp\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which developer has the most contribution in ATS domain? Contributions includes code contributions and reviews.\",\n",
    "        \"query\": \"\"\"\n",
    "          MATCH (d:Developer)\n",
    "          OPTIONAL MATCH (d)-[:DEVELOPER_CONTRIBUTED_TO]->(pr:PR)\n",
    "          WHERE \"ATS\" IN pr.domains\n",
    "          WITH d, COUNT(pr) AS CodeContributions\n",
    "          OPTIONAL MATCH (d)-[:DEVELOPER_REVIEWED]->(pr:PR)\n",
    "          WHERE \"ATS\" IN pr.domains\n",
    "          WITH d, CodeContributions, COUNT(pr) AS Reviews\n",
    "          RETURN d.name AS Developer, (CodeContributions + Reviews) AS TotalContributions\n",
    "          ORDER BY TotalContributions DESC\n",
    "          LIMIT 1\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which developers contributed to pull request 'PR1'?\",\n",
    "        \"query\": \"\"\"\n",
    "          MATCH (d:Developer)-[:DEVELOPER_CONTRIBUTED_TO]->(pr:PR {id: \"PR1\"})\n",
    "          RETURN d.name AS Developer, pr.title AS PullRequest\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which developers have a review score higher than 90?\",\n",
    "        \"query\": \"\"\"\n",
    "          MATCH (d:Developer)\n",
    "          WHERE d.review_score > 90\n",
    "          RETURN d.name AS Developer, d.review_score AS ReviewScore\n",
    "          ORDER BY d.review_score DESC\n",
    "        \"\"\",\n",
    "    },\n",
    "     {\n",
    "        \"question\": \"Which developers are members of the 'Konoha' team?\",\n",
    "        \"query\": \"\"\"\n",
    "          MATCH (d:Developer)-[:IS_MEMBER_OF_TEAM]->(t:Team {name: \"Konoha\"})\n",
    "          RETURN d.name AS Developer, t.name AS Team\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which developers have a skill level of 5 in JavaScript?\",\n",
    "        \"query\": \"\"\"\n",
    "          MATCH (d:Developer)-[r:DEVELOPER_HAS_SKILL]->(s:Skill)\n",
    "          WHERE s.name = \"JavaScript\" AND r.level = 5\n",
    "          RETURN d.name AS Developer, s.name AS Skill, r.level AS SkillLevel\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Find top 3 developers to review a PR created by Nhi, with tech stack Ruby, and domain ATS, SmartMatch file change: file4.rb and file2.py\",\n",
    "        \"query\": \"\"\"\n",
    "            WITH [\n",
    "                \"Ruby\"\n",
    "            ] AS techInput,\n",
    "            [\"ATS\", \"SmartMatch\"] AS prDomains, \n",
    "            [\"file4.rb\", \"file2.py\"] AS fileChanges, \n",
    "            [\"nhi\"] AS prContributors\n",
    "\n",
    "            MATCH (dev:Developer)\n",
    "            WHERE NOT dev.id IN prContributors\n",
    "\n",
    "            OPTIONAL MATCH (dev)-[hs:DEVELOPER_HAS_SKILL]->(skill:Skill)\n",
    "            WHERE skill.name IN techInput\n",
    "            WITH dev, prDomains, fileChanges,\n",
    "                SUM(COALESCE(hs.level, 0)) AS actualSkillScore,\n",
    "                COUNT(skill) AS matchedSkills,\n",
    "                SIZE(techInput) * 5 AS maxSkillScore\n",
    "            WITH dev, prDomains, fileChanges,\n",
    "                (TOFLOAT(actualSkillScore) / COALESCE(maxSkillScore, 1)) AS skillMatchScore\n",
    "\n",
    "            OPTIONAL MATCH (dev)-[contributed:DEVELOPER_CONTRIBUTED_TO]->(pr:PR)-[:PR_AFFECTS_FILE]->(file:File)\n",
    "            WHERE file.name IN fileChanges\n",
    "            WITH dev, prDomains, fileChanges, skillMatchScore,\n",
    "                COUNT(DISTINCT CASE WHEN file IS NOT NULL THEN file END) AS contributionCount\n",
    "            WITH dev, prDomains, fileChanges, skillMatchScore,\n",
    "                CASE \n",
    "                    WHEN contributionCount IS NULL THEN 0 \n",
    "                    WHEN contributionCount = 0 THEN 0 \n",
    "                    WHEN contributionCount = 1 THEN 0.55 \n",
    "                    WHEN contributionCount = 2 THEN 0.75 \n",
    "                    ELSE 1.0 \n",
    "                END AS workHistoryScore\n",
    "\n",
    "            OPTIONAL MATCH (dev)-[reviewed:DEVELOPER_REVIEWED]->(pr:PR)-[:PR_AFFECTS_FILE]->(file:File)\n",
    "            WHERE file.name IN fileChanges\n",
    "            WITH dev, prDomains, fileChanges, skillMatchScore, workHistoryScore,\n",
    "                COALESCE(AVG(reviewed.review_score), 0) AS reviewQualityScore\n",
    "\n",
    "            WITH dev, prDomains, fileChanges, skillMatchScore, workHistoryScore, reviewQualityScore,\n",
    "                dev.code_review_frequent * 0.2 AS reviewFrequencyScore,\n",
    "                dev.availability * 0.2 AS availabilityScore\n",
    "\n",
    "            OPTIONAL MATCH (dev)-[:IS_MEMBER_OF_TEAM|LEADER_OF]->(team:Team)\n",
    "            WITH dev, prDomains, skillMatchScore, workHistoryScore, reviewQualityScore, \n",
    "                reviewFrequencyScore, availabilityScore,\n",
    "                COUNT(DISTINCT CASE WHEN ANY(domain IN team.domains WHERE domain IN prDomains) THEN 1 END) AS domainMatches,\n",
    "                SIZE(prDomains) AS totalDomains\n",
    "            WITH dev, skillMatchScore, workHistoryScore, reviewQualityScore, \n",
    "                reviewFrequencyScore, availabilityScore,\n",
    "                (TOFLOAT(domainMatches) / COALESCE(totalDomains, 1)) AS domainKnowledgeScore\n",
    "\n",
    "            OPTIONAL MATCH (dev)-[:MENTOR]->(mentee:Developer)\n",
    "            WITH dev, skillMatchScore, workHistoryScore, reviewQualityScore, \n",
    "                reviewFrequencyScore, availabilityScore, domainKnowledgeScore,\n",
    "                CASE WHEN mentee IS NOT NULL THEN 1.0 ELSE 0.5 END AS orgPriorityScore\n",
    "\n",
    "            WITH dev,\n",
    "                (skillMatchScore * 0.2) +\n",
    "                (workHistoryScore * 0.2) +\n",
    "                (reviewQualityScore * 0.2) +\n",
    "                (reviewFrequencyScore * 0.1) +\n",
    "                (availabilityScore * 0.1) +\n",
    "                (domainKnowledgeScore * 0.1) +\n",
    "                (orgPriorityScore * 0.05) AS totalScore,\n",
    "                skillMatchScore, workHistoryScore, reviewQualityScore, \n",
    "                reviewFrequencyScore, availabilityScore, domainKnowledgeScore\n",
    "\n",
    "            RETURN dev.id AS developer, dev.name AS name, totalScore, skillMatchScore, workHistoryScore, reviewQualityScore, \n",
    "                reviewFrequencyScore, availabilityScore, domainKnowledgeScore\n",
    "            ORDER BY totalScore DESC\n",
    "            LIMIT 3\n",
    "        \"\"\",\n",
    "    }\n",
    "]\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples, OpenAIEmbeddings(), Neo4jVector, k=4, input_keys=[\"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "text2cypher_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"Given an input question, convert it to a Cypher query. No pre-amble.\"\n",
    "                \"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\"\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.\n",
    "Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\n",
    "Here is the schema information\n",
    "{schema}\n",
    "\n",
    "Below are a number of examples of questions and their corresponding Cypher queries.\n",
    "\n",
    "{fewshot_examples}\n",
    "\n",
    "User input: {question}\n",
    "Cypher query:\"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "text2cypher_chain = text2cypher_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def generate_cypher(state: OverallState) -> OverallState:\n",
    "    \"\"\"\n",
    "    Generates a cypher statement based on the provided schema and user input\n",
    "    \"\"\"\n",
    "    NL = \"\\n\"\n",
    "    fewshot_examples = (NL * 2).join(\n",
    "        [\n",
    "            f\"Question: {el['question']}{NL}Cypher:{el['query']}\"\n",
    "            for el in example_selector.select_examples(\n",
    "                {\"question\": state.get(\"question\")}\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    generated_cypher = text2cypher_chain.invoke(\n",
    "        {\n",
    "            \"question\": state.get(\"question\"),\n",
    "            \"fewshot_examples\": fewshot_examples,\n",
    "            \"schema\": graph.schema,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\"cypher_statement\": generated_cypher, \"steps\": [\"generate_cypher\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "validate_cypher_system = \"\"\"\n",
    "You are a Cypher expert reviewing a statement written by a junior developer.\n",
    "\"\"\"\n",
    "\n",
    "validate_cypher_user = \"\"\"You must check the following:\n",
    "* Are there any syntax errors in the Cypher statement?\n",
    "* Are there any missing or undefined variables in the Cypher statement?\n",
    "* Are any node labels missing from the schema?\n",
    "* Ignore any missing relationship types and properties.\n",
    "* Ignore some inoptimal points in the Cypher statement.\n",
    "* Just need to make sure the query is syntactically correct.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\n",
    "The Cypher statement is:\n",
    "{cypher}\n",
    "\n",
    "Make sure you don't make any mistakes!\"\"\"\n",
    "\n",
    "validate_cypher_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            validate_cypher_system,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (validate_cypher_user),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class Property(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a filter condition based on a specific node property in a graph in a Cypher statement.\n",
    "    \"\"\"\n",
    "\n",
    "    node_label: str = Field(\n",
    "        description=\"The label of the node to which this property belongs.\"\n",
    "    )\n",
    "    property_key: str = Field(description=\"The key of the property being filtered.\")\n",
    "    property_value: str = Field(\n",
    "        description=\"The value that the property is being matched against.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ValidateCypherOutput(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the validation result of a Cypher query's output,\n",
    "    including any errors and applied filters.\n",
    "    \"\"\"\n",
    "\n",
    "    errors: Optional[List[str]] = Field(\n",
    "        description=\"A list of syntax or semantical errors in the Cypher statement. Always explain the discrepancy between schema and Cypher statement\"\n",
    "    )\n",
    "    filters: Optional[List[Property]] = Field(\n",
    "        description=\"A list of property-based filters applied in the Cypher statement.\"\n",
    "    )\n",
    "\n",
    "\n",
    "validate_cypher_chain = validate_cypher_prompt | llm.with_structured_output(\n",
    "    ValidateCypherOutput\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j.chains.graph_qa.cypher_utils import CypherQueryCorrector, Schema\n",
    "\n",
    "# Cypher query corrector is experimental\n",
    "corrector_schema = [\n",
    "    Schema(el[\"start\"], el[\"type\"], el[\"end\"])\n",
    "    for el in graph.structured_schema.get(\"relationships\")\n",
    "]\n",
    "cypher_query_corrector = CypherQueryCorrector(corrector_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j.exceptions import CypherSyntaxError\n",
    "\n",
    "\n",
    "def validate_cypher(state: OverallState) -> OverallState:\n",
    "    \"\"\"\n",
    "    Validates the Cypher statements and maps any property values to the database.\n",
    "    \"\"\"\n",
    "    # errors = []\n",
    "    # mapping_errors = []\n",
    "    # # Check for syntax errors\n",
    "    # try:\n",
    "    #     graph.query(f\"EXPLAIN {state.get('cypher_statement')}\")\n",
    "    # except CypherSyntaxError as e:\n",
    "    #     errors.append(e.message)\n",
    "    # # Experimental feature for correcting relationship directions\n",
    "    # corrected_cypher = cypher_query_corrector(state.get(\"cypher_statement\"))\n",
    "    # if not corrected_cypher:\n",
    "    #     errors.append(\"The generated Cypher statement doesn't fit the graph schema\")\n",
    "    # if not corrected_cypher == state.get(\"cypher_statement\"):\n",
    "    #     print(\"Relationship direction was corrected\")\n",
    "    # # Use LLM to find additional potential errors and get the mapping for values\n",
    "    # print(validate_cypher_chain, 'validate_cypher_chain')\n",
    "    # llm_output = validate_cypher_chain.invoke(\n",
    "    #     {\n",
    "    #         \"question\": state.get(\"question\"),\n",
    "    #         \"schema\": graph.schema,\n",
    "    #         \"cypher\": state.get(\"cypher_statement\"),\n",
    "    #     }\n",
    "    # )\n",
    "    # print(llm_output, 'llm_output')\n",
    "    # if llm_output.errors:\n",
    "    #     errors.extend(llm_output.errors)\n",
    "    # if llm_output.filters:\n",
    "    #     for filter in llm_output.filters:\n",
    "    #         # Do mapping only for string values\n",
    "    #         if (\n",
    "    #             not [\n",
    "    #                 prop\n",
    "    #                 for prop in graph.structured_schema[\"node_props\"][\n",
    "    #                     filter.node_label\n",
    "    #                 ]\n",
    "    #                 if prop[\"property\"] == filter.property_key\n",
    "    #             ][0][\"type\"]\n",
    "    #             == \"STRING\"\n",
    "    #         ):\n",
    "    #             continue\n",
    "    #         mapping = graph.query(\n",
    "    #             f\"MATCH (n:{filter.node_label}) WHERE toLower(n.`{filter.property_key}`) = toLower($value) RETURN 'yes' LIMIT 1\",\n",
    "    #             {\"value\": filter.property_value},\n",
    "    #         )\n",
    "    #         if not mapping:\n",
    "    #             print(\n",
    "    #                 f\"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}\"\n",
    "    #             )\n",
    "    #             mapping_errors.append(\n",
    "    #                 f\"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}\"\n",
    "    #             )\n",
    "    # if mapping_errors:\n",
    "    #     next_action = \"end\"\n",
    "    # elif errors:\n",
    "    #     next_action = \"correct_cypher\"\n",
    "    # else:\n",
    "    #     next_action = \"execute_cypher\"\n",
    "\n",
    "    return {\n",
    "        \"next_action\": \"execute_cypher\",\n",
    "        \"cypher_statement\": state.get(\"cypher_statement\"),\n",
    "        \"cypher_errors\": [],\n",
    "        \"steps\": [\"validate_cypher\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_cypher_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are a Cypher expert reviewing a statement written by a junior developer. \"\n",
    "                \"You need to correct the Cypher statement based on the provided errors. No pre-amble.\"\n",
    "                \"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!\"\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"Check for invalid syntax or semantics and return a corrected Cypher statement.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Note: Do not include any explanations or apologies in your responses.\n",
    "Do not wrap the response in any backticks or anything else.\n",
    "Respond with a Cypher statement only!\n",
    "\n",
    "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "\n",
    "The question is:\n",
    "{question}\n",
    "\n",
    "The Cypher statement is:\n",
    "{cypher}\n",
    "\n",
    "The errors are:\n",
    "{errors}\n",
    "\n",
    "Corrected Cypher statement: \"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "correct_cypher_chain = correct_cypher_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def correct_cypher(state: OverallState) -> OverallState:\n",
    "    \"\"\"\n",
    "    Correct the Cypher statement based on the provided errors.\n",
    "    \"\"\"\n",
    "    corrected_cypher = correct_cypher_chain.invoke(\n",
    "        {\n",
    "            \"question\": state.get(\"question\"),\n",
    "            \"errors\": state.get(\"cypher_errors\"),\n",
    "            \"cypher\": state.get(\"cypher_statement\"),\n",
    "            \"schema\": graph.schema,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"next_action\": \"validate_cypher\",\n",
    "        \"cypher_statement\": corrected_cypher,\n",
    "        \"steps\": [\"correct_cypher\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_results = \"I couldn't find any relevant information in the database\"\n",
    "\n",
    "\n",
    "def execute_cypher(state: OverallState) -> OverallState:\n",
    "    \"\"\"\n",
    "    Executes the given Cypher statement.\n",
    "    \"\"\"\n",
    "\n",
    "    records = graph.query(state.get(\"cypher_statement\"))\n",
    "\n",
    "    return {\n",
    "        \"database_records\": records if records else no_results,\n",
    "        \"next_action\": \"end\",\n",
    "        \"steps\": [\"execute_cypher\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"\"\"Use the following results retrieved from a database to provide\n",
    "a succinct, definitive answer to the user's question.\n",
    "\n",
    "Respond as if you are answering the question directly.\n",
    "\n",
    "Results: {results}\n",
    "Question: {question}\"\"\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "generate_final_chain = generate_final_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def generate_final_answer(state: OverallState) -> OutputState:\n",
    "    \"\"\"\n",
    "    Decides if the question is related to code contribution.\n",
    "    \"\"\"\n",
    "    final_answer = generate_final_chain.invoke(\n",
    "        {\"question\": state.get(\"question\"), \"results\": state.get(\"database_records\")}\n",
    "    )\n",
    "    return {\"answer\": final_answer, \"steps\": [\"generate_final_answer\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardrails_condition(\n",
    "    state: OverallState,\n",
    ") -> Literal[\"generate_cypher\", \"generate_final_answer\"]:\n",
    "    if state.get(\"next_action\") == \"end\":\n",
    "        return \"generate_final_answer\"\n",
    "    elif state.get(\"next_action\") == \"code contribution\":\n",
    "        return \"generate_cypher\"\n",
    "\n",
    "\n",
    "def validate_cypher_condition(\n",
    "    state: OverallState,\n",
    ") -> Literal[\"generate_final_answer\", \"correct_cypher\", \"execute_cypher\"]:\n",
    "    if state.get(\"next_action\") == \"end\":\n",
    "        return \"generate_final_answer\"\n",
    "    elif state.get(\"next_action\") == \"correct_cypher\":\n",
    "        return \"correct_cypher\"\n",
    "    elif state.get(\"next_action\") == \"execute_cypher\":\n",
    "        return \"execute_cypher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "langgraph = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "langgraph.add_node(guardrails)\n",
    "langgraph.add_node(generate_cypher)\n",
    "langgraph.add_node(validate_cypher)\n",
    "langgraph.add_node(correct_cypher)\n",
    "langgraph.add_node(execute_cypher)\n",
    "langgraph.add_node(generate_final_answer)\n",
    "\n",
    "langgraph.add_edge(START, \"guardrails\")\n",
    "langgraph.add_conditional_edges(\n",
    "    \"guardrails\",\n",
    "    guardrails_condition,\n",
    ")\n",
    "langgraph.add_edge(\"generate_cypher\", \"validate_cypher\")\n",
    "langgraph.add_conditional_edges(\n",
    "    \"validate_cypher\",\n",
    "    validate_cypher_condition,\n",
    ")\n",
    "langgraph.add_edge(\"execute_cypher\", \"generate_final_answer\")\n",
    "langgraph.add_edge(\"correct_cypher\", \"validate_cypher\")\n",
    "langgraph.add_edge(\"generate_final_answer\", END)\n",
    "\n",
    "langgraph = langgraph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m langgraph\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFind 3 developers to review a PR created by Nhi, with tech stack Ruby, and domain ATS, SmartMatch file change: file4.rb and file2.py\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1936\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1935\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1936\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1937\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1938\u001b[0m     config,\n\u001b[1;32m   1939\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1940\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1941\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1942\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1943\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1945\u001b[0m ):\n\u001b[1;32m   1946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1947\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1656\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1652\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1656\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1657\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1658\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1659\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1660\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1661\u001b[0m         ):\n\u001b[1;32m   1662\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1663\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langgraph/pregel/runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     run_with_retry(\n\u001b[1;32m    168\u001b[0m         t,\n\u001b[1;32m    169\u001b[0m         retry_policy,\n\u001b[1;32m    170\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    171\u001b[0m             CONFIG_KEY_SEND: partial(writer, t),\n\u001b[1;32m    172\u001b[0m             CONFIG_KEY_CALL: partial(call, t),\n\u001b[1;32m    173\u001b[0m         },\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langgraph/utils/runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m )\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[108], line 39\u001b[0m, in \u001b[0;36mguardrails\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mguardrails\u001b[39m(state: InputState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OverallState:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    Decides if the question is related to code contribution or not.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     guardrails_output \u001b[38;5;241m=\u001b[39m guardrails_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: state\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m)})\n\u001b[1;32m     40\u001b[0m     database_records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m guardrails_output\u001b[38;5;241m.\u001b[39mdecision \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_core/runnables/base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   5355\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   5356\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   5357\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   5358\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    634\u001b[0m                 m,\n\u001b[1;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    853\u001b[0m         )\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:689\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 689\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    860\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    861\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    862\u001b[0m             {\n\u001b[1;32m    863\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    864\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    865\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m    866\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    867\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    868\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    869\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    870\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    871\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    872\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    873\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    874\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m    875\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    876\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    877\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m    878\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    879\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m    880\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    881\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    882\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    883\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    884\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m    885\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    886\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    887\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    888\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    889\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    890\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    891\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    892\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    893\u001b[0m             },\n\u001b[1;32m    894\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    895\u001b[0m         ),\n\u001b[1;32m    896\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    897\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    898\u001b[0m         ),\n\u001b[1;32m    899\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    900\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    901\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    902\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    958\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    959\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    960\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    961\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    962\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    963\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1047\u001b[0m         input_options,\n\u001b[1;32m   1048\u001b[0m         cast_to,\n\u001b[1;32m   1049\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1050\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1051\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1052\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1053\u001b[0m     )\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1096\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1097\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1098\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1099\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1100\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1101\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1047\u001b[0m         input_options,\n\u001b[1;32m   1048\u001b[0m         cast_to,\n\u001b[1;32m   1049\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1050\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1051\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1052\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1053\u001b[0m     )\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1096\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1097\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1098\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1099\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1100\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1101\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\u001b[0mDuring task with name 'guardrails' and id 'ee9467e2-a3fa-2eca-befc-cbcb931d5ab5'"
     ]
    }
   ],
   "source": [
    "langgraph.invoke({\"question\": \"Find 3 developers to review a PR created by Nhi, with tech stack Ruby, and domain ATS, SmartMatch file change: file4.rb and file2.py\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
